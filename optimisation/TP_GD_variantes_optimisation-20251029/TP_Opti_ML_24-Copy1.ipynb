{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efbe2c9-c6dc-4249-b34e-b7165d2c097c",
   "metadata": {},
   "source": [
    "# TP : Optimisation pour l'apprentissage machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da3502f-8417-4d4d-b564-c3b6e4ac7259",
   "metadata": {},
   "source": [
    "- MAsters 1 : MLSD-FI-FA\n",
    "- Année académique 2024/2025\n",
    "- Enseignant : Lazhar Labiod\n",
    "- Adresse :\n",
    "    - Centre Borelli – Université de Paris Cité\n",
    "- Mail : lazhar.labiod@u-paris.fr\n",
    "---\n",
    "- Remise du projet :\n",
    "**Vous devez déposer votre notebook  (ou script python) sur Moodle  au plus tard le mercredi 29 octobre  avant 18h. (Important : 3 étudiants maximum par groupe)**.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a09228-47b4-49e7-b195-c171d8c9eaf4",
   "metadata": {},
   "source": [
    "## Comparaison des méthodes d'optimisation (GD et variantes)\n",
    "---\n",
    "L'objectif est de  comparer différentes méthodes d'optimisation basées sur la descente de gradient ainsi que la méthode de Newton pour minimiser une fonction de coût, comme la régression logistique, sur un ensemble de données. Le projet consisterait à :\n",
    "\n",
    "- Implémenter différentes variantes de la descente de gradient et Newton, telles que :\n",
    "    - Descente de gradient standard (Batch Gradient Descent, BGD)\n",
    "    - Descente de gradient stochastique (Stochastic Gradient Descent, SGD)\n",
    "    - Descente de gradient par mini-lots (Mini-Batch Gradient Descent)\n",
    "    - Descente de gradient avec momentum\n",
    "    - Newton\n",
    "- Comparaison des Méthodes\n",
    "Comparer les performances des différentes méthodes d'optimisation sur les mêmes jeux de données (2 jeux différents). Les critères de comparaison peuvent inclure :\n",
    "\n",
    "    - Le nombre d'itérations nécessaires pour atteindre la convergence.\n",
    "    - Le coût final atteint par la fonction de coût.\n",
    "    - La précision finale sur l'ensemble de test.\n",
    "      \n",
    "- Analyse de la Sensibilité aux Hyperparamètres\n",
    "    - Taux d'Apprentissage : Étudiez l'impact de différents taux d'apprentissage sur la vitesse de convergence et la qualité de la solution obtenue.\n",
    "    - Taille du Mini-Lot (pour Mini-Batch GD) : Testez différentes tailles de mini-lots pour voir leur effet sur la stabilité des mises à jour et la convergence.\n",
    "    - Facteurs de Momentum (β pour Momentum) : Analysez comment les valeurs de ces hyperparamètres affectent les performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1339c5d-d5bc-4c04-be45-29bb01bd3c36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4496746-7226-4e65-ad77-68d28ffcdc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
